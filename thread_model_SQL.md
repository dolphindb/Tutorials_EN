# Overview of Threading Model in DolphinDB 

A distributed database is a complex system where read and write operations often require the coordination of multiple nodes. Understanding the threading model in the DolphinDB distributed database gives us a better insight into the configurations and performance tuning in DolphinDB.

This tutorial takes a distributed SQL query execution as an example to examine the flows of data and the various thread pools they go through during the process. 


- [Overview of Threading Model in DolphinDB](#overview-of-threading-model-in-dolphindb)
  - [1. Before We Start: What are the Node Types?](#1-before-we-start-what-are-the-node-types)
  - [2. Thread Types](#2-thread-types)
  - [3. Process of a Multi-threaded SQL Query](#3-process-of-a-multi-threaded-sql-query)
  - [4. Configurations and Performance Tuning](#4-configurations-and-performance-tuning)
  - [5. Configuration Parameters](#5-configuration-parameters)

## 1. Before We Start: What are the Node Types?

- **controller**

A controller checks the heartbeats of agent nodes, data nodes and compute nodes to monitor the node status and manages the metadata and transactions in the distributed file system (DFS). 

- **data node**

A data node stores data and performs the computational work. 

- **compute node** (1.30.14/2.00.1 and later versions)

Compute nodes only perform the computational work and don't store any data. Compute nodes are designed for computationally intensive operations such as streaming, distributed joins, and machine learning. Use [`loadTable`](https://dolphindb.com/help/FunctionsandCommands/FunctionReferences/l/loadTable.html) to load data to a compute node for computational work. With compute nodes in the cluster, you can separate storage and compute by submitting all write requests to the data nodes and all read requests to the compute nodes.

## 2. Thread Types

- **worker**

A thread that handles regular interactive tasks. 

A worker receives requests from the client and breaks down each request into subtasks. Based on the locations of the partitions involved in the subtasks, the worker assigns the subtasks to local and/or remote executors. The worker also executes local subtasks itself.

- **secondary worker**

Subtasks sent to the remote nodes are handled by the secondary workers on those nodes. This is to avoid deadlocks caused by circular dependencies of tasks on the involved nodes.

- **local executor**

A thread that executes subtasks on local machines assigned by workers.

A worker breaks down a task into a number of subtasks and adds them to the task queues. Each local executor can only process one subtask from the local task queue at a time. Computational tasks generated by the parallel-processing functions such as [`ploop`](https://dolphindb.com/help/Functionalprogramming/TemplateFunctions/loopPloop.html) and [`peach`](https://www.dolphindb.com/help/Functionalprogramming/TemplateFunctions/each.html) are also handled by the local executors.

- **remote executor**

An independent thread that sends subtasks in the remote task queue to the remote nodes.

- **batch job worker**

A thread that handles batch jobs. Batch jobs are submitted with the function [`submitJob`](https://www.dolphindb.com/help/FunctionsandCommands/FunctionReferences/s/submitJob.html) or [`submitJobEx`](https://www.dolphindb.com/help/FunctionsandCommands/FunctionReferences/s/submitJobEx.html). A batch job worker will be destroyed after being idle for over 60 seconds.

- **web worker**

A thread that handles HTTP requests.

DolphinDB provides the Web-based Cluster Manager to interact with cluster nodes. Requests submitted through the web interface are processed by the web workers. 

- **dynamic worker**

When all workers are busy, the system will create dynamic workers to process incoming jobs. Depending on how busy the system is with concurrent tasks, a total of three groups of dynamic workers can be created with *maxDynamicWorker* dynamic threads in each group. A dynamic worker will be destroyed after being idle for 60 seconds. 

- **infra worker**

When high availability is enabled for controllers or streaming, the system will create infra workers for data communication between nodes in the raft group. 

- **urgent worker**

A thread that only handles the time-sensitive system-level tasks such as logins ([`login`](https://www.dolphindb.com/help/FunctionsandCommands/CommandsReferences/l/login.html)) and job cancellations ([`cancelJob`](https://www.dolphindb.com/help/FunctionsandCommands/CommandsReferences/c/cancelJob.html)/[`cancelConsoleJob`](https://www.dolphindb.com/help/FunctionsandCommands/CommandsReferences/c/cancelConsoleJob.html)).

- **diskIO workers**

Set by the configuration parameter *diskIOConcurrencyLevel*. *diskIOConcurrencyLevel* = 0 means the threads handling the current tasks will read and write to the disk directly. When *diskIOConcurrencyLevel* > 0, the system will create the specified number of threads to read and write to the disk concurrently. For more information about *diskIOConcurrencyLevel*, see [5. Configuration Parameters](#5-configuration-parameters).

## 3. Process of a Multi-threaded SQL Query

A SQL query can be sent to any data node or compute node in the cluster. The node that receives the request acts as the coordinator for this operation.

This example illustrates the execution of a SQL query to introduce the threads that are involved in the process. 

![01.thread_model_SQL](./images/thread_model_SQL/01.thread_model_SQL.png)

**step 1. The DolphinDB client sends a query to the coordinator**

In this example, the coordinator is a data node. The following is the query for this example:

```
select avg(price) from loadTable("dfs://database", "table") where date between 2021.01.01 : 2021.12.31 group by date
```

Suppose the query involves 300 partitions in total and the data are evenly distributed on datanode1, datanode2, and datanode3 (i.e., each data node contains 100 of the queried partitions).

The DolphinDB client serializes the query and sends it to datanode1 using the TCP protocol.

**step 2.** **datanode1 receives the query**

Datanode1 assigns a worker to deserialize and parse the request from the client. If itâ€™s a SQL query, the worker sends a request to the controller for metadata about all the partitions involved in the query. This worker is dedicated to this query and will remain busy until the query is complete. 

**step 3. The controller receives the request from datanode1**

The controller starts a worker to deserialize and parse the request from datanode1. The worker then serializes the requested partition information and sends it back to datanode1 using TCP. 

**step 4. datanode1 gets the requested information from the controller**

The worker dedicated to the query deserializes and parses the partition information. It then adds the 100 subtasks involving only the local partitions to the local task queue on datanode1. The other subtasks are organized into two packages (marked as datanode2 and datanode3 respectively) and added to the remote task queue on datanode1.

**step 5 (1). The local worker and local executors consume from the local task queue**

The dedicated worker and the local executors on datanode1 now handle the subtasks in the local task queue in parallel. This explains why the concurrent processing capability of DolphinDB is largely determined by the maximum number of workers and local executors (set by the configuration parameters *workerNum* and *localExecutors*).

**step 5 (2) (3). The remote executors send remote tasks to remote nodes**

Meanwhile, after serializing the tasks in the remote task queue, the remote executors on datanode1 send the tasks to datanode2 and datanode3 using TCP.

**step 6 (1) (2). The remote nodes receive the tasks**

Datanode2 and datanode3 each allocate a secondary worker to deserialize and parse the tasks, adding 100 subtasks to the local task queue on each node.

**step 7 (1) (2). The secondary worker and local executors on remote nodes consume from the local task queue**

The dedicated secondary workers and local executors on datanode2 and datanode3 handle the subtasks in the local task queue in parallel. This explains why the concurrent processing capability of DolphinDB is also affected by the maximum number of secondary workers (set by configuration parameter *secondaryWorkerNum*).

**step 8 (1) (2). Remote nodes send the intermediate results back to datanode1**

When datanode2 and datanode3 complete the computation, the dedicated secondary workers serialize the results and send them back to datanode1 using TCP. 

**step 9. datanode1 returns the final result of the query and sends it back to the client**

The dedicated worker on datanode1 deserializes the intermediate results from the remote nodes. Based on the intermediate results, the worker generates the final result of the query, serializes it, and sends it to the DolphinDB client using TCP.

The client then deserializes the data and prints the final result of the SQL query.

**Data node as the coordinator vs. compute node as the coordinator:**

> \- Unlike data nodes, compute nodes do not store any data. When a compute node receives a query, it parses the query and gets the information of all the partitions involved in this query from the controller. It then breaks down the query into subtasks and assigns all of the subtasks to the nodes storing the involved partitions. After collecting the intermediate results from each node, the compute node calculates the final result and sends it to the client. 
>
> \- To write a large amount of real-time data, it is recommended to submit all SQL queries to compute nodes. This helps to reduce the computational load on data nodes by decoupling storage and compute.

## 4. Configurations and Performance Tuning

In the example above, data transfer with TCP occurred 8 times during the SQL query, 2 of which are between the DolphinDB server and the client. If your queries usually involve a large amount of data and are latency-sensitive, consider the following tips for performance optimization:

- Use 10 Gigabit Ethernet for communication between cluster nodes, and between nodes and clients.
- Tune the configuration parameters according to the recommendations in Section 5.
- Increase the number of physical disks per node. Concurrent reads of partitions are faster when these partitions are stored across more disks.
- Optimize your SQL queries: Add partitioning columns to the *where* condition to enable partition pruning and avoid a full table scan. 
- When querying a large amount of data, compress the query result of your API requests to improve data transmission efficiency. Data will be compressed before they are downloaded from the server. See the code sample below in JAVA:

```
//When setting up the API connection, set the third parameter (compress) to true to enable compression algorithm                       
  DBConnection connection = new DBConnection(false, false, true);        
  connection.connect(HOST, PORT, "admin", "123456");        
  BasicTable basicTable = (BasicTable) connection.run("select * from loadTable(\"dfs://database\", \"table\")");
```

- Increase the maximum number of CPU cores and memory size (both are specified in your DolphinDB license) to increase the concurrent processing capability of DolphinDB.

## 5. Configuration Parameters

| Thread Type      | Configuration Parameter | Default Value           | Recommended Configuration                                                                                                 |
|------------------|-------------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------------|
| worker           | workerNum               | The number of CPU cores | Whichever is smaller between the number of CPU cores specified in the license and the number of CPU cores on your machine |
| local executor   | localExecutors          | workerNum-1             |                                                                                                                           |
| remote executor  | remoteExecutors         | 1                       | The number of cluster nodes - 1 *                                                                                         |
| batch job worker | maxBatchJobWorker       | workerNum               |                                                                                                                           |
| web worker       | webWorkerNum            | 1                       | Set 4 on the controller, 1 on the data nodes **                                                                            |
| secondary worker | secondaryWorkerNum      | workerNum               |                                                                                                                           |
| dynamic worker   | maxDynamicWorker        | workerNum               |                                                                                                                           |
| infra worker     | infraWorkerNum          | 2                       |                                                                                                                           |
| urgent worker    | urgentWorkerNum         | 1                       |                                                                                                                           |
| diskIO worker    | diskIOConcurrencyLevel  | 1                       | Separate configuration for HDDs and SSDs ***                                                                               |


Note:

\* For single-server clusters or clusters with only one data node, *remoteExecutors* is not required.

** In most cases queries are rarely submitted via the web interface to interact with DolphinDB nodes.

*** For HDDs, it is recommended to set *diskIOConcurrencyLevel* to the number of volumes configured on the node (specified by the configuration parameter *volumes*). For SSDs, *diskIOConcurrencyLevel* = 0
